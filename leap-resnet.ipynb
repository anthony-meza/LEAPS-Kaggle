{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1106370",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-29T04:02:44.808047Z",
     "iopub.status.busy": "2024-05-29T04:02:44.807130Z",
     "iopub.status.idle": "2024-05-29T04:02:56.666608Z",
     "shell.execute_reply": "2024-05-29T04:02:56.665612Z"
    },
    "papermill": {
     "duration": 11.868814,
     "end_time": "2024-05-29T04:02:56.669177",
     "exception": false,
     "start_time": "2024-05-29T04:02:44.800363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.handlers import FastaiLRFinder\n",
    "\n",
    "# generator1 = torch.Generator().manual_seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Supported Device: \", DEVICE)\n",
    "\n",
    "# Decide random state\n",
    "torch.manual_seed(42)\n",
    "torch.set_default_dtype(torch.float32) #this is a bug, if using ADAM optimizer need to explicit in all operations. Can't just set default to float64\n",
    "DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b23a17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:02:56.680472Z",
     "iopub.status.busy": "2024-05-29T04:02:56.679931Z",
     "iopub.status.idle": "2024-05-29T04:02:56.699720Z",
     "shell.execute_reply": "2024-05-29T04:02:56.698797Z"
    },
    "papermill": {
     "duration": 0.027614,
     "end_time": "2024-05-29T04:02:56.701759",
     "exception": false,
     "start_time": "2024-05-29T04:02:56.674145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c48085b6930>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define ResNet architecture without Batch Normalization\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features, dtype=DTYPE),\n",
    "            nn.ReLU(inplace=True).double(),\n",
    "            nn.Linear(out_features, out_features, dtype=DTYPE),\n",
    "            nn.ReLU(inplace=True).double(),\n",
    "            nn.Linear(out_features, out_features, dtype=DTYPE),\n",
    "        )\n",
    "        if (in_features != out_features):\n",
    "            self.fc = nn.Linear(in_features, out_features, dtype=DTYPE)\n",
    "        else:\n",
    "            self.fc = nn.Identity(dtype=DTYPE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x) + self.layer(x)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, num_blocks):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        layers = [ResidualBlock(in_features, hidden_features)]\n",
    "        layers += [ResidualBlock(hidden_features, hidden_features) for _ in range(num_blocks-2)]\n",
    "        layers.append(ResidualBlock(hidden_features, out_features))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(out_features, out_features, dtype=DTYPE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.network(x)\n",
    "        return self.fc(out)\n",
    "# He initialization function\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight, nonlinearity='relu')  # He initialization\n",
    "        if m.bias is not None:\n",
    "            init.zeros_(m.bias)\n",
    "            \n",
    "def r2_score_custom(y_pred:torch.Tensor, y_true:torch.Tensor) -> float:\n",
    "    \n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    \n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    \n",
    "    return r2.item()\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c9b6a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:02:56.713114Z",
     "iopub.status.busy": "2024-05-29T04:02:56.712741Z",
     "iopub.status.idle": "2024-05-29T04:02:56.717402Z",
     "shell.execute_reply": "2024-05-29T04:02:56.716473Z"
    },
    "papermill": {
     "duration": 0.012732,
     "end_time": "2024-05-29T04:02:56.719440",
     "exception": false,
     "start_time": "2024-05-29T04:02:56.706708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_STD = 1e-8\n",
    "SCHEDULER_PATIENCE = 3\n",
    "SCHEDULER_FACTOR = 10**(-0.5)\n",
    "PATIENCE = 6\n",
    "PRINT_FREQ = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea3d692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:02:56.730706Z",
     "iopub.status.busy": "2024-05-29T04:02:56.730414Z",
     "iopub.status.idle": "2024-05-29T04:03:33.965278Z",
     "shell.execute_reply": "2024-05-29T04:03:33.964360Z"
    },
    "papermill": {
     "duration": 37.247959,
     "end_time": "2024-05-29T04:03:33.972339",
     "exception": false,
     "start_time": "2024-05-29T04:02:56.724380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 s, sys: 15.3 s, total: 1min 5s\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import each file\n",
    "# temporarily set n_rows for saving time\n",
    "n_rows = 500_000\n",
    "folder = 'leap-atmospheric-physics-ai-climsim'\n",
    "df_train = pl.read_csv('../input/'+folder+'/train.csv', n_rows=n_rows).to_pandas()\n",
    "df_sub = pl.read_csv('../input/'+folder+'/sample_submission.csv', n_rows=2).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba7230c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:03:33.983953Z",
     "iopub.status.busy": "2024-05-29T04:03:33.983272Z",
     "iopub.status.idle": "2024-05-29T04:03:43.332930Z",
     "shell.execute_reply": "2024-05-29T04:03:43.331914Z"
    },
    "papermill": {
     "duration": 9.358064,
     "end_time": "2024-05-29T04:03:43.335370",
     "exception": false,
     "start_time": "2024-05-29T04:03:33.977306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "weights = df_sub.iloc[0, 1:].values.astype(np.float64)\n",
    "weights = torch.tensor(weights, dtype=DTYPE)\n",
    "\n",
    "# Prepare data for training\n",
    "x_train = torch.tensor(1 * df_train.iloc[:, 1:557].values, dtype=DTYPE)\n",
    "y_train = torch.tensor(1 * df_train.iloc[:, 557:].values, dtype=DTYPE)\n",
    "y_train = y_train * weights\n",
    "\n",
    "# Normalization of training data\n",
    "mx = x_train.mean(axis=0)\n",
    "sx = np.maximum(x_train.std(axis=0), MIN_STD)\n",
    "x_train = (x_train - mx.reshape(1,-1)) / sx.reshape(1,-1)\n",
    "\n",
    "my = y_train.mean(axis=0)\n",
    "sy = np.maximum(np.sqrt((y_train*y_train).mean(axis=0)), MIN_STD)\n",
    "y_train = (y_train - my.reshape(1,-1)) / sy.reshape(1,-1)\n",
    "\n",
    "\n",
    "# Create a dataset and shuffle it\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "y_t = y_train[train_dataset.indices, :].to(DEVICE, dtype = DTYPE)\n",
    "y_v = y_train[val_dataset.indices, :].to(DEVICE, dtype = DTYPE)\n",
    "\n",
    "SS_res_train = torch.sum((y_t - y_t.mean())**2)\n",
    "SS_res_val = torch.sum((y_v - y_v.mean())**2)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1_000, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1_000, shuffle=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c451cca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:03:43.346908Z",
     "iopub.status.busy": "2024-05-29T04:03:43.346560Z",
     "iopub.status.idle": "2024-05-29T04:21:53.612553Z",
     "shell.execute_reply": "2024-05-29T04:21:53.611516Z"
    },
    "papermill": {
     "duration": 1090.279597,
     "end_time": "2024-05-29T04:21:53.620097",
     "exception": false,
     "start_time": "2024-05-29T04:03:43.340500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Train Loss: 0.4273, Val Loss: 0.4747\n",
      "Epoch [5/50], Train $R^2$: 0.4479, Val $R^2$: 0.4205\n",
      "Epoch [10/50], Train Loss: 0.5042, Val Loss: 0.4785\n",
      "Epoch [10/50], Train $R^2$: 0.3484, Val $R^2$: 0.4158\n",
      "Epoch [15/50], Train Loss: 0.3697, Val Loss: 0.4600\n",
      "Epoch [15/50], Train $R^2$: 0.5223, Val $R^2$: 0.4385\n",
      "Epoch [20/50], Train Loss: 0.3515, Val Loss: 0.4523\n",
      "Epoch [20/50], Train $R^2$: 0.5458, Val $R^2$: 0.4478\n",
      "Epoch [25/50], Train Loss: 0.3757, Val Loss: 0.4575\n",
      "Epoch [25/50], Train $R^2$: 0.5145, Val $R^2$: 0.4414\n",
      "Epoch [30/50], Train Loss: 0.3217, Val Loss: 0.4840\n",
      "Epoch [30/50], Train $R^2$: 0.5842, Val $R^2$: 0.4091\n",
      "Epoch [35/50], Train Loss: 0.3169, Val Loss: 0.4785\n",
      "Epoch [35/50], Train $R^2$: 0.5905, Val $R^2$: 0.4158\n",
      "Epoch [40/50], Train Loss: 0.3015, Val Loss: 0.6187\n",
      "Epoch [40/50], Train $R^2$: 0.6104, Val $R^2$: 0.2446\n",
      "Epoch [45/50], Train Loss: 0.2859, Val Loss: 0.4706\n",
      "Epoch [45/50], Train $R^2$: 0.6306, Val $R^2$: 0.4255\n",
      "Epoch [50/50], Train Loss: 0.2859, Val Loss: 0.4623\n",
      "Epoch [50/50], Train $R^2$: 0.6305, Val $R^2$: 0.4356\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = x_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "hidden_dim = np.int64(np.mean([input_dim, output_dim]).round()) #hidden layers is the mean of the number of inputs and outputs\n",
    "num_res_blocks = 7\n",
    "\n",
    "# Create ResNet model\n",
    "model = ResNet(input_dim, output_dim, hidden_dim, num_res_blocks)\n",
    "model = model.to(DEVICE, dtype = DTYPE)\n",
    "\n",
    "# Apply the He weight initialization (does not work well)\n",
    "# model.apply(weights_init)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Training settings\n",
    "criterion = nn.MSELoss(reduction = \"mean\").to(DEVICE, dtype = DTYPE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE)\n",
    "\n",
    "# lr_finder.apply_suggested_lr(optimizer)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_in, batch_out in train_loader:\n",
    "        \n",
    "        batch_in = batch_in.to(DEVICE, dtype = DTYPE)\n",
    "        batch_out = batch_out.to(DEVICE, dtype = DTYPE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_in)\n",
    "        \n",
    "        loss = criterion(predictions, batch_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_in, batch_out in val_loader:\n",
    "            batch_in = batch_in.to(DEVICE, dtype = DTYPE)\n",
    "            batch_out = batch_out.to(DEVICE, dtype = DTYPE)\n",
    "\n",
    "            predictions = model(batch_in)\n",
    "            loss = criterion(predictions, batch_out)\n",
    "            val_loss += loss.item()\n",
    "#     scheduler.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        Rsq_train = (train_loss * train_loader.batch_size * output_dim) / SS_res_train; Rsq_train = 1 - Rsq_train\n",
    "        Rsq_test = (val_loss * val_loader.batch_size * output_dim) / SS_res_val; Rsq_test = 1 - Rsq_test\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train $R^2$: {Rsq_train:.4f}, Val $R^2$: {Rsq_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9600b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:21:53.633832Z",
     "iopub.status.busy": "2024-05-29T04:21:53.632896Z",
     "iopub.status.idle": "2024-05-29T04:21:53.675286Z",
     "shell.execute_reply": "2024-05-29T04:21:53.674491Z"
    },
    "papermill": {
     "duration": 0.051775,
     "end_time": "2024-05-29T04:21:53.677585",
     "exception": false,
     "start_time": "2024-05-29T04:21:53.625810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a16236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:21:53.690712Z",
     "iopub.status.busy": "2024-05-29T04:21:53.690383Z",
     "iopub.status.idle": "2024-05-29T04:21:53.776313Z",
     "shell.execute_reply": "2024-05-29T04:21:53.775267Z"
    },
    "papermill": {
     "duration": 0.094789,
     "end_time": "2024-05-29T04:21:53.778424",
     "exception": false,
     "start_time": "2024-05-29T04:21:53.683635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(input_dim, output_dim, hidden_dim, num_res_blocks)\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79859b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:21:53.791703Z",
     "iopub.status.busy": "2024-05-29T04:21:53.791378Z",
     "iopub.status.idle": "2024-05-29T04:21:54.013376Z",
     "shell.execute_reply": "2024-05-29T04:21:54.012479Z"
    },
    "papermill": {
     "duration": 0.2311,
     "end_time": "2024-05-29T04:21:54.015379",
     "exception": false,
     "start_time": "2024-05-29T04:21:53.784279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ec6b5a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:21:54.028713Z",
     "iopub.status.busy": "2024-05-29T04:21:54.028399Z",
     "iopub.status.idle": "2024-05-29T04:22:38.781490Z",
     "shell.execute_reply": "2024-05-29T04:22:38.780616Z"
    },
    "papermill": {
     "duration": 44.762451,
     "end_time": "2024-05-29T04:22:38.784030",
     "exception": false,
     "start_time": "2024-05-29T04:21:54.021579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pl.read_csv('../input/'+folder+'/test.csv')\n",
    "df_test = df_test.to_pandas()\n",
    "df_test = df_test.set_index(\"sample_id\")\n",
    "\n",
    "df_subm = pl.read_csv('../input/'+folder+'/sample_submission.csv')\n",
    "df_subm = df_subm.to_pandas()\n",
    "df_subm = df_subm.set_index(\"sample_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697a9b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:22:38.798030Z",
     "iopub.status.busy": "2024-05-29T04:22:38.797434Z",
     "iopub.status.idle": "2024-05-29T04:22:45.403775Z",
     "shell.execute_reply": "2024-05-29T04:22:45.402779Z"
    },
    "papermill": {
     "duration": 6.616132,
     "end_time": "2024-05-29T04:22:45.406639",
     "exception": false,
     "start_time": "2024-05-29T04:22:38.790507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "x_test = torch.tensor(1 * df_test.to_numpy()).to(DEVICE, dtype=DTYPE)\n",
    "x_test = (x_test - mx.to(DEVICE, dtype=DTYPE).reshape(1,-1)) / sx.to(DEVICE, dtype=DTYPE).reshape(1,-1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "        y_est = model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8617ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:22:45.420843Z",
     "iopub.status.busy": "2024-05-29T04:22:45.420013Z",
     "iopub.status.idle": "2024-05-29T04:22:45.439105Z",
     "shell.execute_reply": "2024-05-29T04:22:45.438129Z"
    },
    "papermill": {
     "duration": 0.028358,
     "end_time": "2024-05-29T04:22:45.441279",
     "exception": false,
     "start_time": "2024-05-29T04:22:45.412921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_NAMES = ['ptend_t_0', 'ptend_t_1', 'ptend_t_2', 'ptend_t_3', 'ptend_t_4', 'ptend_t_5', 'ptend_t_6', 'ptend_t_7', 'ptend_t_8', 'ptend_t_9', 'ptend_t_10', 'ptend_t_11', 'ptend_t_12', 'ptend_t_13', 'ptend_t_14', 'ptend_t_15', 'ptend_t_16', 'ptend_t_17', 'ptend_t_18', 'ptend_t_19', 'ptend_t_20', 'ptend_t_21', 'ptend_t_22', 'ptend_t_23', 'ptend_t_24', 'ptend_t_25', 'ptend_t_26', 'ptend_t_27', 'ptend_t_28', 'ptend_t_29', 'ptend_t_30', 'ptend_t_31', 'ptend_t_32', 'ptend_t_33', 'ptend_t_34', 'ptend_t_35', 'ptend_t_36', 'ptend_t_37', 'ptend_t_38', 'ptend_t_39', 'ptend_t_40', 'ptend_t_41', 'ptend_t_42', 'ptend_t_43', 'ptend_t_44', 'ptend_t_45', 'ptend_t_46', 'ptend_t_47', 'ptend_t_48', 'ptend_t_49', 'ptend_t_50', 'ptend_t_51', 'ptend_t_52', 'ptend_t_53', 'ptend_t_54', 'ptend_t_55', 'ptend_t_56', 'ptend_t_57', 'ptend_t_58', 'ptend_t_59', 'ptend_q0001_0', 'ptend_q0001_1', 'ptend_q0001_2', 'ptend_q0001_3', 'ptend_q0001_4', 'ptend_q0001_5', 'ptend_q0001_6', 'ptend_q0001_7', 'ptend_q0001_8', 'ptend_q0001_9', 'ptend_q0001_10', 'ptend_q0001_11', 'ptend_q0001_12', 'ptend_q0001_13', 'ptend_q0001_14', 'ptend_q0001_15', 'ptend_q0001_16', 'ptend_q0001_17', 'ptend_q0001_18', 'ptend_q0001_19', 'ptend_q0001_20', 'ptend_q0001_21', 'ptend_q0001_22', 'ptend_q0001_23', 'ptend_q0001_24', 'ptend_q0001_25', 'ptend_q0001_26', 'ptend_q0001_27', 'ptend_q0001_28', 'ptend_q0001_29', 'ptend_q0001_30', 'ptend_q0001_31', 'ptend_q0001_32', 'ptend_q0001_33', 'ptend_q0001_34', 'ptend_q0001_35', 'ptend_q0001_36', 'ptend_q0001_37', 'ptend_q0001_38', 'ptend_q0001_39', 'ptend_q0001_40', 'ptend_q0001_41', 'ptend_q0001_42', 'ptend_q0001_43', 'ptend_q0001_44', 'ptend_q0001_45', 'ptend_q0001_46', 'ptend_q0001_47', 'ptend_q0001_48', 'ptend_q0001_49', 'ptend_q0001_50', 'ptend_q0001_51', 'ptend_q0001_52', 'ptend_q0001_53', 'ptend_q0001_54', 'ptend_q0001_55', 'ptend_q0001_56', 'ptend_q0001_57', 'ptend_q0001_58', 'ptend_q0001_59', 'ptend_q0002_0', 'ptend_q0002_1', 'ptend_q0002_2', 'ptend_q0002_3', 'ptend_q0002_4', 'ptend_q0002_5', 'ptend_q0002_6', 'ptend_q0002_7', 'ptend_q0002_8', 'ptend_q0002_9', 'ptend_q0002_10', 'ptend_q0002_11', 'ptend_q0002_12', 'ptend_q0002_13', 'ptend_q0002_14', 'ptend_q0002_15', 'ptend_q0002_16', 'ptend_q0002_17', 'ptend_q0002_18', 'ptend_q0002_19', 'ptend_q0002_20', 'ptend_q0002_21', 'ptend_q0002_22', 'ptend_q0002_23', 'ptend_q0002_24', 'ptend_q0002_25', 'ptend_q0002_26', 'ptend_q0002_27', 'ptend_q0002_28', 'ptend_q0002_29', 'ptend_q0002_30', 'ptend_q0002_31', 'ptend_q0002_32', 'ptend_q0002_33', 'ptend_q0002_34', 'ptend_q0002_35', 'ptend_q0002_36', 'ptend_q0002_37', 'ptend_q0002_38', 'ptend_q0002_39', 'ptend_q0002_40', 'ptend_q0002_41', 'ptend_q0002_42', 'ptend_q0002_43', 'ptend_q0002_44', 'ptend_q0002_45', 'ptend_q0002_46', 'ptend_q0002_47', 'ptend_q0002_48', 'ptend_q0002_49', 'ptend_q0002_50', 'ptend_q0002_51', 'ptend_q0002_52', 'ptend_q0002_53', 'ptend_q0002_54', 'ptend_q0002_55', 'ptend_q0002_56', 'ptend_q0002_57', 'ptend_q0002_58', 'ptend_q0002_59', 'ptend_q0003_0', 'ptend_q0003_1', 'ptend_q0003_2', 'ptend_q0003_3', 'ptend_q0003_4', 'ptend_q0003_5', 'ptend_q0003_6', 'ptend_q0003_7', 'ptend_q0003_8', 'ptend_q0003_9', 'ptend_q0003_10', 'ptend_q0003_11', 'ptend_q0003_12', 'ptend_q0003_13', 'ptend_q0003_14', 'ptend_q0003_15', 'ptend_q0003_16', 'ptend_q0003_17', 'ptend_q0003_18', 'ptend_q0003_19', 'ptend_q0003_20', 'ptend_q0003_21', 'ptend_q0003_22', 'ptend_q0003_23', 'ptend_q0003_24', 'ptend_q0003_25', 'ptend_q0003_26', 'ptend_q0003_27', 'ptend_q0003_28', 'ptend_q0003_29', 'ptend_q0003_30', 'ptend_q0003_31', 'ptend_q0003_32', 'ptend_q0003_33', 'ptend_q0003_34', 'ptend_q0003_35', 'ptend_q0003_36', 'ptend_q0003_37', 'ptend_q0003_38', 'ptend_q0003_39', 'ptend_q0003_40', 'ptend_q0003_41', 'ptend_q0003_42', 'ptend_q0003_43', 'ptend_q0003_44', 'ptend_q0003_45', 'ptend_q0003_46', 'ptend_q0003_47', 'ptend_q0003_48', 'ptend_q0003_49', 'ptend_q0003_50', 'ptend_q0003_51', 'ptend_q0003_52', 'ptend_q0003_53', 'ptend_q0003_54', 'ptend_q0003_55', 'ptend_q0003_56', 'ptend_q0003_57', 'ptend_q0003_58', 'ptend_q0003_59', 'ptend_u_0', 'ptend_u_1', 'ptend_u_2', 'ptend_u_3', 'ptend_u_4', 'ptend_u_5', 'ptend_u_6', 'ptend_u_7', 'ptend_u_8', 'ptend_u_9', 'ptend_u_10', 'ptend_u_11', 'ptend_u_12', 'ptend_u_13', 'ptend_u_14', 'ptend_u_15', 'ptend_u_16', 'ptend_u_17', 'ptend_u_18', 'ptend_u_19', 'ptend_u_20', 'ptend_u_21', 'ptend_u_22', 'ptend_u_23', 'ptend_u_24', 'ptend_u_25', 'ptend_u_26', 'ptend_u_27', 'ptend_u_28', 'ptend_u_29', 'ptend_u_30', 'ptend_u_31', 'ptend_u_32', 'ptend_u_33', 'ptend_u_34', 'ptend_u_35', 'ptend_u_36', 'ptend_u_37', 'ptend_u_38', 'ptend_u_39', 'ptend_u_40', 'ptend_u_41', 'ptend_u_42', 'ptend_u_43', 'ptend_u_44', 'ptend_u_45', 'ptend_u_46', 'ptend_u_47', 'ptend_u_48', 'ptend_u_49', 'ptend_u_50', 'ptend_u_51', 'ptend_u_52', 'ptend_u_53', 'ptend_u_54', 'ptend_u_55', 'ptend_u_56', 'ptend_u_57', 'ptend_u_58', 'ptend_u_59', 'ptend_v_0', 'ptend_v_1', 'ptend_v_2', 'ptend_v_3', 'ptend_v_4', 'ptend_v_5', 'ptend_v_6', 'ptend_v_7', 'ptend_v_8', 'ptend_v_9', 'ptend_v_10', 'ptend_v_11', 'ptend_v_12', 'ptend_v_13', 'ptend_v_14', 'ptend_v_15', 'ptend_v_16', 'ptend_v_17', 'ptend_v_18', 'ptend_v_19', 'ptend_v_20', 'ptend_v_21', 'ptend_v_22', 'ptend_v_23', 'ptend_v_24', 'ptend_v_25', 'ptend_v_26', 'ptend_v_27', 'ptend_v_28', 'ptend_v_29', 'ptend_v_30', 'ptend_v_31', 'ptend_v_32', 'ptend_v_33', 'ptend_v_34', 'ptend_v_35', 'ptend_v_36', 'ptend_v_37', 'ptend_v_38', 'ptend_v_39', 'ptend_v_40', 'ptend_v_41', 'ptend_v_42', 'ptend_v_43', 'ptend_v_44', 'ptend_v_45', 'ptend_v_46', 'ptend_v_47', 'ptend_v_48', 'ptend_v_49', 'ptend_v_50', 'ptend_v_51', 'ptend_v_52', 'ptend_v_53', 'ptend_v_54', 'ptend_v_55', 'ptend_v_56', 'ptend_v_57', 'ptend_v_58', 'ptend_v_59', 'cam_out_NETSW', 'cam_out_FLWDS', 'cam_out_PRECSC', 'cam_out_PRECC', 'cam_out_SOLS', 'cam_out_SOLL', 'cam_out_SOLSD', 'cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f434ea5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-29T04:22:45.454488Z",
     "iopub.status.busy": "2024-05-29T04:22:45.454165Z",
     "iopub.status.idle": "2024-05-29T04:23:13.711421Z",
     "shell.execute_reply": "2024-05-29T04:23:13.710121Z"
    },
    "papermill": {
     "duration": 28.268673,
     "end_time": "2024-05-29T04:23:13.715957",
     "exception": false,
     "start_time": "2024-05-29T04:22:45.447284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Test File\n",
      "CPU times: user 31.2 s, sys: 11.5 s, total: 42.7 s\n",
      "Wall time: 28.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (625_000, 369)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample_id</th><th>ptend_t_0</th><th>ptend_t_1</th><th>ptend_t_2</th><th>ptend_t_3</th><th>ptend_t_4</th><th>ptend_t_5</th><th>ptend_t_6</th><th>ptend_t_7</th><th>ptend_t_8</th><th>ptend_t_9</th><th>ptend_t_10</th><th>ptend_t_11</th><th>ptend_t_12</th><th>ptend_t_13</th><th>ptend_t_14</th><th>ptend_t_15</th><th>ptend_t_16</th><th>ptend_t_17</th><th>ptend_t_18</th><th>ptend_t_19</th><th>ptend_t_20</th><th>ptend_t_21</th><th>ptend_t_22</th><th>ptend_t_23</th><th>ptend_t_24</th><th>ptend_t_25</th><th>ptend_t_26</th><th>ptend_t_27</th><th>ptend_t_28</th><th>ptend_t_29</th><th>ptend_t_30</th><th>ptend_t_31</th><th>ptend_t_32</th><th>ptend_t_33</th><th>ptend_t_34</th><th>ptend_t_35</th><th>&hellip;</th><th>ptend_v_31</th><th>ptend_v_32</th><th>ptend_v_33</th><th>ptend_v_34</th><th>ptend_v_35</th><th>ptend_v_36</th><th>ptend_v_37</th><th>ptend_v_38</th><th>ptend_v_39</th><th>ptend_v_40</th><th>ptend_v_41</th><th>ptend_v_42</th><th>ptend_v_43</th><th>ptend_v_44</th><th>ptend_v_45</th><th>ptend_v_46</th><th>ptend_v_47</th><th>ptend_v_48</th><th>ptend_v_49</th><th>ptend_v_50</th><th>ptend_v_51</th><th>ptend_v_52</th><th>ptend_v_53</th><th>ptend_v_54</th><th>ptend_v_55</th><th>ptend_v_56</th><th>ptend_v_57</th><th>ptend_v_58</th><th>ptend_v_59</th><th>cam_out_NETSW</th><th>cam_out_FLWDS</th><th>cam_out_PRECSC</th><th>cam_out_PRECC</th><th>cam_out_SOLS</th><th>cam_out_SOLL</th><th>cam_out_SOLSD</th><th>cam_out_SOLLD</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;test_169651&quot;</td><td>0.121571</td><td>-0.493538</td><td>-0.083397</td><td>-0.299344</td><td>-0.616368</td><td>-0.859764</td><td>-0.921237</td><td>-0.908393</td><td>-1.004644</td><td>-0.989049</td><td>-0.885063</td><td>-0.794898</td><td>-0.789876</td><td>-0.604074</td><td>-0.257513</td><td>0.089419</td><td>0.022484</td><td>0.26962</td><td>0.125336</td><td>-0.060211</td><td>0.103092</td><td>-0.28003</td><td>-0.47849</td><td>-0.61908</td><td>-0.71699</td><td>-0.866627</td><td>-1.012016</td><td>-0.901568</td><td>-0.909278</td><td>-0.838012</td><td>-0.788432</td><td>-0.701559</td><td>-0.628489</td><td>-0.567237</td><td>-0.570446</td><td>-0.432552</td><td>&hellip;</td><td>-0.087128</td><td>-0.084063</td><td>-0.186856</td><td>-0.133083</td><td>-0.024662</td><td>0.049324</td><td>-0.055132</td><td>0.030734</td><td>0.178703</td><td>0.220072</td><td>0.170313</td><td>0.057763</td><td>-0.0694</td><td>-0.025766</td><td>0.094257</td><td>-0.029559</td><td>-0.067677</td><td>-0.01339</td><td>0.118927</td><td>0.152087</td><td>0.09077</td><td>0.087334</td><td>-0.104625</td><td>-0.219307</td><td>-0.427933</td><td>-0.206692</td><td>0.129918</td><td>0.282764</td><td>0.295666</td><td>0.005056</td><td>4.911753</td><td>0.055448</td><td>-0.089787</td><td>0.000189</td><td>0.090915</td><td>0.195587</td><td>0.269291</td></tr><tr><td>&quot;test_524862&quot;</td><td>0.144425</td><td>-0.374275</td><td>-0.142555</td><td>-0.245702</td><td>-0.649414</td><td>-0.872517</td><td>-0.872439</td><td>-0.821513</td><td>-0.921117</td><td>-0.957905</td><td>-0.900094</td><td>-0.815395</td><td>-0.857144</td><td>-0.770775</td><td>-0.475102</td><td>-0.116852</td><td>-0.269393</td><td>0.191344</td><td>0.054855</td><td>-0.167499</td><td>-0.017105</td><td>-0.351726</td><td>-0.343926</td><td>-0.414926</td><td>-0.45212</td><td>-0.471876</td><td>-0.666639</td><td>-0.667187</td><td>-0.714477</td><td>-0.677714</td><td>-0.67082</td><td>-0.626127</td><td>-0.570778</td><td>-0.523806</td><td>-0.544749</td><td>-0.471153</td><td>&hellip;</td><td>-0.421759</td><td>-0.289868</td><td>-0.070149</td><td>0.221203</td><td>0.31819</td><td>0.281163</td><td>0.008129</td><td>0.04753</td><td>0.199146</td><td>0.208815</td><td>0.141451</td><td>0.01645</td><td>-0.071009</td><td>-0.000312</td><td>0.070846</td><td>-0.11899</td><td>-0.144341</td><td>-0.07479</td><td>0.058668</td><td>0.028752</td><td>-0.027213</td><td>0.007125</td><td>-0.091738</td><td>-0.157638</td><td>-0.263367</td><td>-0.082796</td><td>0.094031</td><td>0.251855</td><td>0.43265</td><td>0.013998</td><td>4.731476</td><td>0.012559</td><td>0.060134</td><td>-0.013782</td><td>0.070702</td><td>0.158087</td><td>0.250763</td></tr><tr><td>&quot;test_634129&quot;</td><td>0.034685</td><td>-0.979847</td><td>-0.361393</td><td>-0.369399</td><td>-0.643838</td><td>-0.954246</td><td>-0.992321</td><td>-0.927706</td><td>-0.888605</td><td>-0.836025</td><td>-0.759955</td><td>-0.750483</td><td>-0.829596</td><td>-0.690155</td><td>-0.344751</td><td>0.148764</td><td>0.191434</td><td>0.484561</td><td>0.0856</td><td>-0.423054</td><td>0.305648</td><td>-0.075834</td><td>-0.509585</td><td>-0.679575</td><td>-0.830861</td><td>-1.018635</td><td>-1.05901</td><td>-0.760274</td><td>-0.840035</td><td>-0.900288</td><td>-0.923677</td><td>-0.878317</td><td>-0.814862</td><td>-0.653375</td><td>-0.694742</td><td>-0.610921</td><td>&hellip;</td><td>0.135022</td><td>0.119046</td><td>-0.057644</td><td>0.010471</td><td>0.092298</td><td>0.061475</td><td>-0.143132</td><td>-0.009314</td><td>0.161412</td><td>0.170543</td><td>0.051311</td><td>-0.055051</td><td>-0.16005</td><td>-0.100986</td><td>0.039179</td><td>-0.16602</td><td>-0.243188</td><td>-0.175802</td><td>-0.083968</td><td>-0.016128</td><td>0.059292</td><td>0.104703</td><td>0.044858</td><td>0.071813</td><td>-0.127641</td><td>-0.143441</td><td>0.115547</td><td>0.342091</td><td>0.179289</td><td>0.070306</td><td>5.299164</td><td>-0.050974</td><td>-0.050818</td><td>0.093235</td><td>0.140507</td><td>0.247853</td><td>0.406688</td></tr><tr><td>&quot;test_403572&quot;</td><td>0.041548</td><td>-0.858429</td><td>-0.337164</td><td>-0.297285</td><td>-0.542609</td><td>-0.845865</td><td>-0.927849</td><td>-0.87988</td><td>-0.851609</td><td>-0.783587</td><td>-0.712319</td><td>-0.679666</td><td>-0.777433</td><td>-0.720369</td><td>-0.545828</td><td>-0.176126</td><td>-0.222048</td><td>0.513194</td><td>0.178578</td><td>-0.369111</td><td>0.196623</td><td>-0.328883</td><td>-0.529524</td><td>-0.709634</td><td>-0.701098</td><td>-0.771716</td><td>-0.999405</td><td>-0.915578</td><td>-0.922318</td><td>-0.946914</td><td>-1.018447</td><td>-0.995175</td><td>-0.874624</td><td>-0.664878</td><td>-0.60899</td><td>-0.483311</td><td>&hellip;</td><td>-0.790158</td><td>-0.769188</td><td>-0.361485</td><td>0.138744</td><td>0.434993</td><td>0.38477</td><td>-0.017797</td><td>0.01763</td><td>0.17562</td><td>0.061629</td><td>-0.162557</td><td>-0.29382</td><td>-0.237584</td><td>-0.060741</td><td>0.026885</td><td>-0.237984</td><td>-0.325212</td><td>-0.3277</td><td>-0.173546</td><td>-0.109202</td><td>-0.006871</td><td>0.092854</td><td>0.061904</td><td>0.168294</td><td>0.220281</td><td>0.44127</td><td>0.288891</td><td>0.032564</td><td>-0.106579</td><td>0.149063</td><td>5.41166</td><td>-0.008102</td><td>0.213669</td><td>0.114002</td><td>0.146753</td><td>0.204655</td><td>0.328838</td></tr><tr><td>&quot;test_484578&quot;</td><td>0.035924</td><td>-0.191551</td><td>-0.117315</td><td>-0.362626</td><td>-0.757245</td><td>-0.938058</td><td>-0.912257</td><td>-0.856244</td><td>-0.940729</td><td>-0.958571</td><td>-0.900171</td><td>-0.82674</td><td>-0.924499</td><td>-0.908343</td><td>-0.633827</td><td>-0.26809</td><td>-0.551024</td><td>-0.047395</td><td>-0.201097</td><td>-0.124972</td><td>-0.101936</td><td>-0.227135</td><td>-0.224664</td><td>-0.412842</td><td>-0.510847</td><td>-0.482195</td><td>-0.656097</td><td>-0.612971</td><td>-0.607863</td><td>-0.579909</td><td>-0.60363</td><td>-0.60261</td><td>-0.593597</td><td>-0.532882</td><td>-0.543089</td><td>-0.498497</td><td>&hellip;</td><td>-0.222083</td><td>-0.019173</td><td>0.185703</td><td>0.272803</td><td>0.084456</td><td>-0.097081</td><td>-0.32376</td><td>-0.131898</td><td>0.054056</td><td>0.006122</td><td>-0.127951</td><td>-0.257673</td><td>-0.353059</td><td>-0.244524</td><td>-0.015824</td><td>-0.084424</td><td>-0.141667</td><td>-0.065276</td><td>0.157946</td><td>0.20176</td><td>0.17372</td><td>0.172483</td><td>0.144349</td><td>0.11837</td><td>0.03652</td><td>0.102754</td><td>0.403372</td><td>0.798422</td><td>-0.214358</td><td>-0.104545</td><td>4.643112</td><td>-0.117599</td><td>0.172421</td><td>-0.112144</td><td>-0.028436</td><td>0.077203</td><td>0.213371</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;test_578220&quot;</td><td>1.665629</td><td>0.934587</td><td>1.656129</td><td>1.800557</td><td>2.008804</td><td>1.81927</td><td>1.511916</td><td>1.418681</td><td>1.386176</td><td>1.452139</td><td>1.649666</td><td>1.760979</td><td>1.538717</td><td>1.515766</td><td>1.700965</td><td>1.800849</td><td>1.095197</td><td>1.302894</td><td>0.634776</td><td>0.461557</td><td>0.58887</td><td>0.222714</td><td>-0.03855</td><td>-0.496347</td><td>-0.752369</td><td>-0.673673</td><td>-0.547946</td><td>-0.150435</td><td>-0.081827</td><td>-0.120177</td><td>-0.340859</td><td>-0.410237</td><td>-0.426514</td><td>-0.395313</td><td>-0.547473</td><td>-0.401415</td><td>&hellip;</td><td>0.569863</td><td>-0.089724</td><td>-0.596331</td><td>-0.473236</td><td>-0.193135</td><td>-0.19237</td><td>-0.467872</td><td>-0.306083</td><td>0.03873</td><td>0.241883</td><td>0.196299</td><td>0.190085</td><td>0.024465</td><td>-0.041255</td><td>-0.088524</td><td>-0.263311</td><td>-0.237688</td><td>-0.291413</td><td>-0.425355</td><td>-0.479348</td><td>-0.4191</td><td>-0.379687</td><td>-0.311419</td><td>0.053758</td><td>0.33551</td><td>0.447775</td><td>0.414967</td><td>0.411115</td><td>0.186206</td><td>1.617283</td><td>4.722603</td><td>-0.116673</td><td>-0.220415</td><td>1.428907</td><td>1.582396</td><td>1.864242</td><td>1.505997</td></tr><tr><td>&quot;test_395695&quot;</td><td>1.797935</td><td>1.000962</td><td>1.36302</td><td>1.827596</td><td>1.785378</td><td>1.556888</td><td>1.297511</td><td>1.25368</td><td>1.364868</td><td>1.59229</td><td>1.760029</td><td>1.615594</td><td>1.227811</td><td>1.191345</td><td>1.558963</td><td>1.892805</td><td>1.521527</td><td>0.919623</td><td>0.390925</td><td>0.613487</td><td>0.675445</td><td>0.054023</td><td>-0.029856</td><td>-0.040625</td><td>-0.126628</td><td>0.015174</td><td>0.201049</td><td>0.459204</td><td>0.487673</td><td>0.336777</td><td>0.058193</td><td>-0.074836</td><td>-0.044974</td><td>0.042491</td><td>-0.070177</td><td>-0.063173</td><td>&hellip;</td><td>-0.494947</td><td>-0.731973</td><td>-0.777929</td><td>-0.421568</td><td>-0.159346</td><td>-0.304992</td><td>-0.561282</td><td>-0.252278</td><td>0.17842</td><td>0.267287</td><td>-0.090311</td><td>-0.351402</td><td>-0.522034</td><td>-0.429972</td><td>-0.203288</td><td>-0.080131</td><td>0.115262</td><td>0.330969</td><td>0.518945</td><td>0.510763</td><td>0.475317</td><td>0.417226</td><td>0.186574</td><td>-0.019249</td><td>-0.137724</td><td>-0.16693</td><td>-0.381467</td><td>-0.295015</td><td>0.133287</td><td>1.36324</td><td>5.271986</td><td>0.170413</td><td>0.202125</td><td>0.889772</td><td>0.963525</td><td>2.528087</td><td>2.537095</td></tr><tr><td>&quot;test_88942&quot;</td><td>1.866313</td><td>0.601136</td><td>1.252318</td><td>1.909749</td><td>1.595144</td><td>1.109962</td><td>0.818317</td><td>0.864053</td><td>0.940947</td><td>1.053709</td><td>1.199302</td><td>1.203791</td><td>1.038793</td><td>1.014131</td><td>1.264531</td><td>1.577348</td><td>1.218242</td><td>0.9285</td><td>0.254418</td><td>0.184826</td><td>0.652216</td><td>0.270428</td><td>0.064573</td><td>-0.091214</td><td>-0.279695</td><td>-0.340706</td><td>-0.256721</td><td>-0.052305</td><td>-0.08679</td><td>-0.156517</td><td>-0.273493</td><td>-0.27501</td><td>-0.181273</td><td>-0.13962</td><td>-0.182384</td><td>-0.170339</td><td>&hellip;</td><td>-1.078028</td><td>-1.074296</td><td>-0.930897</td><td>-0.349326</td><td>0.167323</td><td>0.27397</td><td>6.4675e-7</td><td>0.136888</td><td>0.283576</td><td>-0.111119</td><td>-0.561287</td><td>-0.665053</td><td>-0.580328</td><td>-0.534242</td><td>-0.474791</td><td>-0.492931</td><td>-0.308875</td><td>-0.0292</td><td>0.274244</td><td>0.408671</td><td>0.634217</td><td>0.815316</td><td>0.653375</td><td>0.54372</td><td>0.418729</td><td>0.299177</td><td>-0.306078</td><td>-0.993655</td><td>-0.03059</td><td>0.985899</td><td>5.273352</td><td>0.081377</td><td>0.125614</td><td>0.720775</td><td>0.814898</td><td>1.86936</td><td>1.857164</td></tr><tr><td>&quot;test_79382&quot;</td><td>1.765403</td><td>0.582719</td><td>1.276469</td><td>1.70894</td><td>1.688532</td><td>1.392906</td><td>1.086345</td><td>1.033766</td><td>0.977871</td><td>1.0446</td><td>1.205514</td><td>1.314674</td><td>1.219362</td><td>1.292598</td><td>1.601423</td><td>1.782444</td><td>1.048538</td><td>0.884014</td><td>0.415635</td><td>0.313838</td><td>0.604612</td><td>0.375494</td><td>-0.024952</td><td>-0.571246</td><td>-0.842123</td><td>-0.812126</td><td>-0.767745</td><td>-0.380228</td><td>-0.338669</td><td>-0.41005</td><td>-0.562026</td><td>-0.475862</td><td>-0.392272</td><td>-0.298594</td><td>-0.484207</td><td>-0.304156</td><td>&hellip;</td><td>-0.985981</td><td>-0.992189</td><td>-1.018759</td><td>-0.617651</td><td>-0.168876</td><td>0.065261</td><td>0.013148</td><td>0.160457</td><td>0.32238</td><td>0.228478</td><td>0.012335</td><td>-0.31616</td><td>-0.998491</td><td>-1.429677</td><td>-1.543581</td><td>-1.241064</td><td>-0.456604</td><td>0.240731</td><td>0.467584</td><td>0.38825</td><td>0.459725</td><td>0.462718</td><td>0.474032</td><td>0.643008</td><td>0.727934</td><td>0.717642</td><td>0.39923</td><td>0.072865</td><td>-0.28812</td><td>1.442247</td><td>4.589394</td><td>-0.110062</td><td>-0.050401</td><td>1.435407</td><td>1.591622</td><td>1.427616</td><td>1.126996</td></tr><tr><td>&quot;test_601350&quot;</td><td>1.778244</td><td>0.572392</td><td>1.205715</td><td>1.639401</td><td>1.448534</td><td>0.966566</td><td>0.605425</td><td>0.587478</td><td>0.604247</td><td>0.717147</td><td>0.924136</td><td>1.091961</td><td>1.009232</td><td>0.999511</td><td>1.268767</td><td>1.464172</td><td>0.668117</td><td>0.477002</td><td>0.405959</td><td>-0.022296</td><td>0.419295</td><td>0.212478</td><td>-0.075462</td><td>-0.469131</td><td>-0.700204</td><td>-0.785698</td><td>-0.925883</td><td>-0.5052</td><td>-0.105844</td><td>0.137326</td><td>0.192784</td><td>0.155602</td><td>-0.020707</td><td>-0.046594</td><td>-0.23245</td><td>-0.200101</td><td>&hellip;</td><td>-1.126338</td><td>-0.678227</td><td>-0.567101</td><td>-0.278758</td><td>-0.078759</td><td>0.057592</td><td>0.049942</td><td>0.186916</td><td>0.220563</td><td>-0.026956</td><td>-0.259219</td><td>-0.40238</td><td>-0.679353</td><td>-0.702798</td><td>-0.671182</td><td>-0.792533</td><td>-0.64807</td><td>-0.266963</td><td>0.046917</td><td>0.135322</td><td>0.309456</td><td>0.430961</td><td>0.532189</td><td>0.745753</td><td>0.812715</td><td>0.82516</td><td>0.344227</td><td>0.130622</td><td>-0.306398</td><td>0.796112</td><td>5.117311</td><td>-0.218304</td><td>0.130936</td><td>0.724047</td><td>0.857785</td><td>1.417345</td><td>1.316548</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (625_000, 369)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ sample_id ┆ ptend_t_0 ┆ ptend_t_1 ┆ ptend_t_2 ┆ … ┆ cam_out_S ┆ cam_out_S ┆ cam_out_S ┆ cam_out_ │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ OLS       ┆ OLL       ┆ OLSD      ┆ SOLLD    │\n",
       "│ str       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ test_1696 ┆ 0.121571  ┆ -0.493538 ┆ -0.083397 ┆ … ┆ 0.000189  ┆ 0.090915  ┆ 0.195587  ┆ 0.269291 │\n",
       "│ 51        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_5248 ┆ 0.144425  ┆ -0.374275 ┆ -0.142555 ┆ … ┆ -0.013782 ┆ 0.070702  ┆ 0.158087  ┆ 0.250763 │\n",
       "│ 62        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_6341 ┆ 0.034685  ┆ -0.979847 ┆ -0.361393 ┆ … ┆ 0.093235  ┆ 0.140507  ┆ 0.247853  ┆ 0.406688 │\n",
       "│ 29        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_4035 ┆ 0.041548  ┆ -0.858429 ┆ -0.337164 ┆ … ┆ 0.114002  ┆ 0.146753  ┆ 0.204655  ┆ 0.328838 │\n",
       "│ 72        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_4845 ┆ 0.035924  ┆ -0.191551 ┆ -0.117315 ┆ … ┆ -0.112144 ┆ -0.028436 ┆ 0.077203  ┆ 0.213371 │\n",
       "│ 78        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ test_5782 ┆ 1.665629  ┆ 0.934587  ┆ 1.656129  ┆ … ┆ 1.428907  ┆ 1.582396  ┆ 1.864242  ┆ 1.505997 │\n",
       "│ 20        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_3956 ┆ 1.797935  ┆ 1.000962  ┆ 1.36302   ┆ … ┆ 0.889772  ┆ 0.963525  ┆ 2.528087  ┆ 2.537095 │\n",
       "│ 95        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_8894 ┆ 1.866313  ┆ 0.601136  ┆ 1.252318  ┆ … ┆ 0.720775  ┆ 0.814898  ┆ 1.86936   ┆ 1.857164 │\n",
       "│ 2         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_7938 ┆ 1.765403  ┆ 0.582719  ┆ 1.276469  ┆ … ┆ 1.435407  ┆ 1.591622  ┆ 1.427616  ┆ 1.126996 │\n",
       "│ 2         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ test_6013 ┆ 1.778244  ┆ 0.572392  ┆ 1.205715  ┆ … ┆ 0.724047  ┆ 0.857785  ┆ 1.417345  ┆ 1.316548 │\n",
       "│ 50        ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "prediction = (y_est * sy.to(DEVICE, dtype=DTYPE).reshape(1,-1)) + my.to(DEVICE, dtype=DTYPE).reshape(1,-1)\n",
    "\n",
    "prediction = prediction.cpu().detach().numpy()\n",
    "print(\"Saving Test File\")\n",
    "df_subm_final = df_subm.astype(np.float64) * 1\n",
    "df_subm_final.loc[:, :] = prediction\n",
    "df_subm_final = df_subm_final.reset_index()\n",
    "df_subm_final = df_subm_final[[\"sample_id\"] + TARGET_NAMES]\n",
    "df_subm_final = pl.from_pandas(df_subm_final)\n",
    "df_subm_final.write_csv(\"submission.csv\")\n",
    "df_subm_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183e5e5",
   "metadata": {
    "papermill": {
     "duration": 0.036782,
     "end_time": "2024-05-29T04:23:13.765192",
     "exception": false,
     "start_time": "2024-05-29T04:23:13.728410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8015876,
     "sourceId": 56537,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1234.755684,
   "end_time": "2024-05-29T04:23:16.708995",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-29T04:02:41.953311",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
